#!/usr/bin/env python
# -*- coding: utf-8; mode: python; -*-

"""Script for doing sense disambiguation of discourse connectives.

"""

##################################################################
# Imports
from __future__ import print_function, unicode_literals

from dsenser import DiscourseSenser, \
    DFLT_MODEL_PATH, DFLT_MODEL_TYPE, \
    SVD, LSTM, MJR, WANG, XGBOOST
from dsenser.constants import ARG1, ARG2, COB, COE, \
    CONNECTIVE, DOC_ID, SENTENCES, TOK_LIST, WORDS

import argparse
import codecs
import glob
import json
import os
import sys


##################################################################
# Variables and Constants
M_TRAIN = "train"
M_TEST = "test"
ENCODING = "utf-8"

ARGS_FNAME = "relations-no-senses.json"
GOLD_FNAME = "relations.json"
PARSE_FNAME = "parses.json"
OUT_FNAME = "output.json"
RAW_DIR = "raw"
WSJ_GLOB = "wsj_*"


##################################################################
# Methods
def _add_cmn_options(a_parser):
    """Add common options to option subparser

    Args:
      a_parser (argparse.ArgumentParser):
        option subparser to which new options should be added

    Returns:
      void:

    """
    a_parser.add_argument("-m", "--model",
                          help="path to the main model (if different from"
                          " default)", type=str, default=DFLT_MODEL_PATH)


def _read_json(a_dir, a_fname, a_enc=ENCODING):
    """Read JSON data from file.

    Args:
      a_dir (str):
        dirname to the input file
      a_fname (str):
        basename of the input file
      a_enc (str):
        input file encoding

    Returns:
      list: input data as list

    """
    fname = os.path.join(a_dir, a_fname or "")
    if a_fname is None or not (os.path.exists(fname) and
                               os.access(fname, os.R_OK)):
        raise RuntimeError("Can't open file '{:s}'.".format(a_fname))
    print("Loading file '{:s}'... ".format(fname), end="", file=sys.stderr)
    ret = []
    with codecs.open(fname, 'r', encoding=a_enc,
                     errors="replace") as ifile:
        for iline in ifile:
            iline = iline.strip()
            if iline:
                ret.append(json.loads(iline))
    print("done", file=sys.stderr)
    return ret


def _read_raw(a_dir, a_enc=ENCODING):
    """Read raw data from directory.

    Args:
      a_dir (str):
       path to directory containing raw data
      a_enc (str):
       encoding of the input files

    Returns:
      dict:
        dictionary with file names as keys and 3-tuple (paragraph number,
        sentence numberm, and sentence string) as values

    """
    ret = dict()
    bfname = ""
    snt_cnt = 0
    prgrph_cnt = -1
    prev_empty = False
    sentences = None
    for fname in glob.iglob(os.path.join(a_dir, WSJ_GLOB)):
        snt_cnt = 0
        prgrph_cnt = -1
        bfname = os.path.basename(bfname)
        sentences = ret[bfname] = []
        print("Loading file '{:s}'... ".format(fname), end="", file=sys.stderr)
        with codecs.open(fname, 'r', ENCODING,
                         errors="replace") as ifile:
            for iline in ifile:
                iline = iline.strip()
                if not iline:
                    if prev_empty:
                        continue
                    prev_empty = True
                    prgrph_cnt += 1
                elif iline == ".START":
                    continue
                else:
                    prev_empty = False
                    sentences.append((prgrph_cnt, snt_cnt, iline))
                    snt_cnt += 1
        print("done", file=sys.stderr)
    return ret


def _get_tok_pos(a_parses):
    """Convert document token indices to 4-tuples.

    Args:
      a_parses (dict):
        sentence parses for discourse relations

    Returns:
      dict: map document token index to 4-tuple

    """
    ret = {}
    doc_w_i = 0
    w_attrs = None
    for doc_id, doc in a_parses.iteritems():
        doc_w_i = 0
        sentences = doc[SENTENCES]
        for s_cnt, isent in enumerate(sentences):
            for w_cnt, w in enumerate(isent[WORDS]):
                w_attrs = w[-1]
                ret[(doc_id, doc_w_i)] = \
                    (w_attrs[COB], w_attrs[COE], doc_w_i, s_cnt, w_cnt)
                doc_w_i += 1
    return ret


def _restore_toks(a_rel, a_key, a_doc_id, a_tok2tpl):
    """Convert document token indices to 4-tuples.

    Args:
      a_rel (dict):
        relation whose token indices should be restored
      a_key (dict):
        key of the field whose token indices should be restored
      a_doc_id (str):
        id of the document to which given item pertains
      a_tok2tpl (dict):
        map document token index to 4-tuple

    Returns:
      void:
        modifies relation in place

    """
    tlist = a_rel[a_key].get(TOK_LIST)
    if tlist is not None:
        a_rel[a_key][TOK_LIST] = [a_tok2tpl[(a_doc_id, t)] for t in tlist]


def _restore_rel_fmt(a_rels, a_parses):
    """Restore token index format from integers to 4-tuples.

    Args:
      a_rels (list):
        input relations whose format should be restored
      a_parses (dict):
        sentence parses for discourse relations

    Returns:
      void:

    """
    idocid = None
    tok2tpl = _get_tok_pos(a_parses)
    for irel in a_rels:
        idocid = irel.get(DOC_ID)
        _restore_toks(irel, ARG1, idocid, tok2tpl)
        _restore_toks(irel, ARG2, idocid, tok2tpl)
        _restore_toks(irel, CONNECTIVE, idocid, tok2tpl)


def _read_dataset(a_dir, a_train=False, a_rel_file=None, a_int_tok=False):
    """Read data from directory.

    Args:
      a_dir (str):
        path to directory containing input data
      a_train (bool):
        flag indicating whether the dataset is used for training
      a_rel_file (None or str):
        relation file or None (if located in the input directory as
        relations-no-senses.json)
      a_int_tok (bool):
        token indices are specified as integer lists

    Returns:
      tuple:
        training rel set and parses

    """
    if not a_dir:
        return None
    # read parses
    with codecs.open(os.path.join(a_dir, PARSE_FNAME), 'r', ENCODING,
                     errors="replace") as ifile:
        print("Loading file '{:s}'... ".format(ifile.name), end="",
              file=sys.stderr)
        parses = json.load(ifile)
        print("done", file=sys.stderr)
    # read relations
    if a_rel_file is not None:
        rels = _read_json(a_dir, a_rel_file)
    elif a_train:
        rels = _read_json(a_dir, GOLD_FNAME)
    else:
        rels = _read_json(a_dir, ARGS_FNAME)
    # convert format if necessary
    if a_int_tok:
        _restore_rel_fmt(rels, parses)
    return [rels, parses]


def main():
    """Main method for doing discourse parsing of CoNLL JSON data.

    No arguments are accepted ``sys.argv`` is parsed in place.

    Returns:
      int:
        0 on success, non-0 otherwise

    """
    # parse arguments
    argparser = argparse.ArgumentParser(description="""
    Determine senses of discourse connectives.""")

    subparsers = argparser.add_subparsers(help="type of operation to perform",
                                          dest="mode")
    # training parser
    parser_train = subparsers.add_parser(M_TRAIN, help=
                                         "train new model on provided data")
    parser_train.add_argument("-d", "--dev", help=
                              "development directory with JSON data",
                              type=str)
    parser_train.add_argument("-g", "--grid-search",
                              help="use grid search for determining the"
                              " hyper-parameters of SVC and XGBoost",
                              action="store_true")
    parser_train.add_argument("--w2v", help="use word2vec embeddings",
                              action="store_true")
    parser_train.add_argument("--lstsq", help="use the least squares method"
                              " to convert generic embeddings to task-specific"
                              " ones", action="store_true")
    parser_train.add_argument("--type", help=("type of the model to train"
                                              "({:d} - SVD, {:d} - LSTM, {:d}"
                                              " - majority, {:d} - Wang, {:d}"
                                              " - XGBOOST), ".format(SVD, LSTM,
                                                                     MJR, WANG,
                                                                     XGBOOST) +
                                              "default = {:d}".format(
                                                  DFLT_MODEL_TYPE)),
                              type=int, action="append", choices=(SVD, LSTM,
                                                                  MJR, WANG,
                                                                  XGBOOST))
    _add_cmn_options(parser_train)
    parser_train.add_argument("input_dir", help="directories containing input"
                              " data", type=str, nargs="+")

    # testing parser
    parser_test = subparsers.add_parser(M_TEST, help="determine senses"
                                        " using pre-trained models")
    _add_cmn_options(parser_test)
    parser_test.add_argument("--int-tok", help="tokens are specified as lists"
                             " of integers", action="store_true")
    parser_test.add_argument("--rel-file", help="file containing input"
                             " relations without senses (if different from"
                             " default)", type=str)
    parser_test.add_argument("input_dir", help="directories containing input"
                             " data", type=str)
    parser_test.add_argument("output_dir", help=
                             "directory for storing output files",
                             type=str)
    args = argparser.parse_args()
    if hasattr(args, "type") and not args.type:
        args.type = [DFLT_MODEL_TYPE]

    # instantiate objects required for processing
    dsenser = None

    if args.mode == M_TRAIN:
        # read data
        dev_set = _read_dataset(args.dev, True)
        rels = []
        parses = {}
        irels = iparses = None
        for idir in args.input_dir:
            irels, iparses = _read_dataset(idir, True)
            rels.extend(irels)
            parses.update(iparses)
        train_set = (rels, parses)
        # determine model type
        mtype = 0
        for itype in args.type:
            mtype |= itype
        # initialize and train the model
        dsenser = DiscourseSenser(None)
        dsenser.train(train_set, mtype, args.model, dev_set,
                      a_grid_search=args.grid_search,
                      a_w2v=args.w2v, a_lstsq=args.lstsq)
    else:
        if not os.path.exists(args.output_dir):
            os.makedirs(args.output_dir)
        elif not os.path.isdir(args.output_dir) or \
                not os.access(args.output_dir, os.W_OK):
                raise RuntimeError("Cannot write to directory '{:s}'.".format(
                    args.output_dir))
        # read test data
        dsenser = DiscourseSenser(args.model)
        test_set = _read_dataset(args.input_dir,
                                 a_rel_file=args.rel_file,
                                 a_int_tok=args.int_tok)
        dsenser.predict(test_set)
        # write results to file
        ofname = os.path.join(args.output_dir, OUT_FNAME)
        with codecs.open(ofname, 'w', ENCODING,
                         errors="replace") as ofile:
            for irel in test_set[0]:
                print(json.dumps(irel), file=ofile)


##################################################################
# Main
if __name__ == "__main__":
    main()
